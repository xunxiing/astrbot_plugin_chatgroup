#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Chat Topic Grouper (æ–‡æœ¬èŠå¤©ä¸»é¢˜åˆ†ç»„)
====================================================

ä¸€é”®æŠŠèŠå¤©è®°å½•åˆ†æˆè‹¥å¹²ä¸»é¢˜ç»„ï¼Œå¹¶ç»™æ¯ä¸ªç»„è‡ªåŠ¨èµ·â€œä¸»é¢˜åâ€ã€‚

âœ… ç‰¹ç‚¹
- æ”¯æŒä¸¤ç§å‘é‡æä¾›æ–¹å¼ï¼š
  1) æœ¬åœ°æ¨¡å‹ï¼ˆsentence-transformersï¼‰ï¼šå¤‡ç”¨ï¼›
  2) **ç¡…åŸºæµåŠ¨ SiliconFlow Embeddings**ï¼šé»˜è®¤æ¨èï¼›æ¨¡å‹ `BAAI/bge-m3`ï¼ˆ4096ç»´ï¼‰ã€‚
- è‡ªåŠ¨é€‰æ‹© KMeans èšç±»ç°‡æ•°ï¼ˆåŸºäº silhouette scoreï¼‰ã€‚
- ä»¥ TFâ€‘IDFï¼ˆå¯é€‰ jieba åˆ†è¯ï¼Œè‹¥æ— åˆ™ç”¨ä¸­æ–‡å­—ç¬¦ nâ€‘gramï¼‰ä¸ºæ¯ä¸ªç°‡ç”Ÿæˆå…³é”®è¯ï¼Œæ‹¼æˆä¸»é¢˜åã€‚
- å¯¼å‡ºä¸¤ä¸ªæ–‡ä»¶ï¼š
  - `messages_with_labels.jsonl`ï¼šæ¯æ¡æ¶ˆæ¯çš„èšç±»æ ‡ç­¾
  - `clusters.json`ï¼šæ¯ä¸ªä¸»é¢˜ç»„çš„å…³é”®ä¿¡æ¯ï¼ˆåç§°ã€å…³é”®è¯ã€ä»£è¡¨æ¶ˆæ¯ç­‰ï¼‰

ğŸ“¦ ä¾èµ–å®‰è£…ï¼ˆä»»é€‰å…¶ä¸€æˆ–éƒ½è£…ï¼‰ï¼š
  pip install scikit-learn emoji tqdm pandas numpy python-dateutil beautifulsoup4
  # å¦‚ä½¿ç”¨æœ¬åœ°å‘é‡ï¼š
  pip install sentence-transformers
  # å¦‚ä½¿ç”¨ jiebaï¼ˆæ›´å¥½çš„ä¸­æ–‡åˆ†è¯ï¼‰ï¼š
  pip install jieba

ğŸ”‘ è‹¥ä½¿ç”¨ç¡…åŸºæµåŠ¨ï¼š
  - éœ€è®¾ç½®ç¯å¢ƒå˜é‡ï¼šSILICONFLOW_API_KEY
  - é»˜è®¤ä½¿ç”¨ base_url: https://api.siliconflow.cn/v1/embeddings

ğŸ’¾ è¾“å…¥æ ¼å¼ï¼ˆJSONLï¼Œæ¯è¡Œä¸€æ¡æ¶ˆæ¯ï¼Œç¤ºä¾‹ï¼‰ï¼š
{
  "id": "m_001",
  "timestamp": "2025-09-20T13:45:12+08:00",
  "user_id": "u_123",
  "user_name": "Alice",
  "text": "åˆé¥­ç‚¹ä»€ä¹ˆï¼ŸğŸ˜‚",
  "reply_to": null,                    # æˆ–æŸæ¡æ¶ˆæ¯ id
  "channel": "dev-group",            # å¯é€‰
  "attachments": [                    # å¯é€‰
    {"type": "file", "name": "è®¾è®¡ç¨¿.pdf", "title": "æ–°ç‰ˆå¯¼èˆª"}
  ],
  "html": null                        # è‹¥åŸå§‹æ˜¯å¯Œæ–‡æœ¬ï¼Œä¿ç•™åŸ HTML ä»¥ä¾¿æ¸…æ´—
}

â–¶ è¿è¡Œï¼š
  python chat_topic_grouper.py \
    --input ./chat.jsonl \
    --provider siliconflow \
    --model "BAAI/bge-m3" \
    --output ./out

  # ä½¿ç”¨æœ¬åœ° ST æ¨¡å‹å¤‡ç”¨ï¼š
  python chat_topic_grouper.py --input ./chat.jsonl --provider local --model paraphrase-multilingual-MiniLM-L12-v2

æ³¨æ„ï¼šæœ¬è„šæœ¬é‡ç‚¹åœ¨â€œèƒ½è·‘ + å·¥ç¨‹æ¸…æ™°â€ã€‚åç»­ä½ è¦åŠ å›¾ç‰‡/OCR/å¤šæ¨¡æ€ï¼Œå¯åœ¨ build_message_text é‡ŒæŠŠå›¾ç‰‡/é™„ä»¶æ‘˜è¦æ‹¼è¿›æ–‡æœ¬å³å¯ã€‚
"""
from __future__ import annotations

import os
import json
import math
import argparse
import logging
import time
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple, Callable

from dotenv import load_dotenv
load_dotenv()  # è‡ªåŠ¨è¯»å– .env æ–‡ä»¶

import numpy as np
import pandas as pd
from tqdm import tqdm
from datetime import datetime
from dateutil import parser as dateparser
from bs4 import BeautifulSoup
import re
import hdbscan
from sklearn.preprocessing import normalize
from sklearn.feature_extraction.text import TfidfVectorizer
from dotenv import load_dotenv

# å¯é€‰ï¼šjieba åˆ†è¯ï¼ˆæ›´é€‚åˆä¸­æ–‡ï¼‰ï¼›è‹¥æ— åˆ™é€€åŒ–åˆ° char ngram
try:
    import jieba  # type: ignore
    HAS_JIEBA = True
except Exception:
    HAS_JIEBA = False

# emoji å¤„ç†ï¼ˆæŠŠ ğŸ˜‚ â†’ :face_with_tears_of_joy:ï¼‰
import emoji

# æœ¬åœ°å¤‡é€‰ï¼šsentence-transformersï¼ˆè‹¥é€‰æ‹© provider=local ä½¿ç”¨ï¼‰
_ST_MODEL = None
try:
    from sentence_transformers import SentenceTransformer  # type: ignore
except Exception:
    SentenceTransformer = None  # noqa: N816

# ------------------------------
# æ•°æ®ç»“æ„
# ------------------------------
@dataclass
class Message:
    id: str
    timestamp: Optional[datetime]
    user_id: Optional[str]
    user_name: Optional[str]
    text: str
    reply_to: Optional[str]
    channel: Optional[str]
    attachments: Optional[List[Dict[str, Any]]]
    html: Optional[str]


# ------------------------------
# å·¥å…·å‡½æ•°ï¼šIO & é¢„å¤„ç†
# ------------------------------

def load_jsonl(path: str) -> List[Dict[str, Any]]:
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                data.append(json.loads(line))
            except json.JSONDecodeError as e:
                logging.warning("è·³è¿‡æ— æ³•è§£æçš„è¡Œ: %s | é”™è¯¯: %s", line[:120], e)
    return data


def read_messages(input_path: str) -> List[Message]:
    items: List[Dict[str, Any]] = []
    if os.path.isdir(input_path):
        for fn in sorted(os.listdir(input_path)):
            if fn.lower().endswith(".jsonl"):
                items.extend(load_jsonl(os.path.join(input_path, fn)))
    else:
        items = load_jsonl(input_path)

    messages: List[Message] = []
    for it in items:
        ts = None
        if it.get("timestamp"):
            try:
                ts = dateparser.parse(it["timestamp"])  # tz-aware ok
            except Exception:
                ts = None
        msg = Message(
            id=str(it.get("id", "")),
            timestamp=ts,
            user_id=it.get("user_id"),
            user_name=it.get("user_name"),
            text=it.get("text", ""),
            reply_to=it.get("reply_to"),
            channel=it.get("channel"),
            attachments=it.get("attachments"),
            html=it.get("html"),
        )
        messages.append(msg)
    return messages


_TAG_RE = re.compile(r"<[^>]+>")
_WHITESPACE_RE = re.compile(r"\s+")


def strip_html(html: str) -> str:
    # æ›´ç¨³å¦¥ï¼šBeautifulSoupï¼›è‹¥å¤±è´¥é€€å›æ­£åˆ™
    try:
        soup = BeautifulSoup(html, "html.parser")
        txt = soup.get_text(separator=" ")
    except Exception:
        txt = _TAG_RE.sub(" ", html)
    txt = _WHITESPACE_RE.sub(" ", txt).strip()
    return txt


def demojize_text(s: str) -> str:
    # ğŸ˜‚ â†’ :face_with_tears_of_joy:
    try:
        return emoji.demojize(s, language='zh')
    except Exception:
        return s

# å¹³å°/å£æ°´è¯ç­‰å™ªå£°æ¸…æ´—
PLATFORM_PATTERNS = [
    r"\[CQ:[^\]]+\]",     # KOOK/QQ é£æ ¼å†…åµŒæ ‡ç­¾
    r"@\S+",              # @æåŠ
    r"https?://\S+",      # URL
    r"[A-Za-z0-9_]{10,}", # é•¿ID/å“ˆå¸Œ
]
PLATFORM_RE = re.compile("|".join(f"(?:{p})" for p in PLATFORM_PATTERNS))

LOW_CONTENT_TOKENS = set(list("å—¯å•Šå“¦å“ˆå‘µâ€¦ï¼Ÿ?~!") + ["å¥½", "è¡Œ", "å¯ä»¥", "ä¸è¡Œ", "æ˜¯çš„", "ok", "OK", "å·²å¤„ç†", "æ”¶åˆ°"])

def strip_platform_artifacts(s: str) -> str:
    s = PLATFORM_RE.sub(" ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def is_low_content(s: str) -> bool:
    # ä»…è¡¨æƒ…/å¾ˆçŸ­/å…¨æ˜¯å£æ°´è¯ â†’ ä½ä¿¡æ¯
    if not s or len(s) < 3:
        return True
    # å¤§é‡ demojize åçš„ :xxx: ä¹Ÿè§†ä¸ºä½ä¿¡æ¯
    if re.fullmatch(r"(?::[a-z0-9_]+:|\W){1,7}", s, flags=re.I):
        return True
    # åªç”±å°‘é‡å£æ°´è¯æ„æˆ
    toks = [t for t in re.split(r"\s+", s) if t]
    if 0 < len(toks) <= 3 and all(t in LOW_CONTENT_TOKENS for t in toks):
        return True
    return False


ANCHOR_PAT = re.compile(r"(https?://\S+|\\[é™„ä»¶:[^\\]]+\\]|`{3,}|\\?|#\\w+)", re.I)
def has_anchor(s: str) -> bool:
    # é“¾æ¥ã€é™„ä»¶å ä½ã€ä»£ç å—ã€é—®å·ã€#è¯é¢˜ â€”â€” ä»»ä¸€å‘½ä¸­å³è®¤ä¸ºæœ‰"é”šç‚¹"
    return bool(ANCHOR_PAT.search(s))


def build_message_text(m: Message) -> str:
    # ä¼˜å…ˆç”¨ html æ¸…æ´—ï¼Œå¦åˆ™ç”¨ text
    base = m.text or ""
    if m.html:
        base = strip_html(m.html)

    base = demojize_text(base)

    # é™„ä»¶æç¤ºï¼šåªåŠ ç®€è¦ä¸Šä¸‹æ–‡ï¼Œä¸è¦å¤ªå¤šå™ªå£°
    att_txts = []
    if m.attachments:
        for a in m.attachments:
            name = (a.get("title") or a.get("name") or a.get("type") or "").strip()
            # å¿½ç•¥åªæœ‰æ‰©å±•åæˆ– 1~2 ä¸ªå­—ç¬¦çš„"ç©ºå"é™„ä»¶ï¼Œä¾‹å¦‚ ".png"
            stem, ext = os.path.splitext(name)
            if not stem or len(stem.strip()) < 2:
                continue
            att_txts.append(f"[é™„ä»¶:{stem}]")
    if att_txts:
        base = f"{base} {' '.join(att_txts)}".strip()

    # å¼•ç”¨/çº¿ç¨‹æç¤ºï¼ˆè‹¥æœ‰ reply_toï¼Œå¯åŠ è½»é‡æç¤ºï¼›å®é™… thread è§£ç¼ ä»¥åå†åšï¼‰
    if m.reply_to:
        base = f"{base} [å›å¤:{m.reply_to}]"
    
    base = strip_platform_artifacts(base)
    # å»æ‰åªç”±è¡¨æƒ…/æ ‡ç‚¹ç»„æˆçš„å ä½
    if re.fullmatch(r"(?::[a-z0-9_]+:|\W){1,}", base, flags=re.I):
        base = ""
    return base.strip()


# ------------------------------
# é‡è¯•å·¥å…·å‡½æ•°
# ------------------------------
def retry_with_backoff(max_retries: int = 4, initial_delay: float = 1.0, backoff_factor: float = 2.0):
    """
    é‡è¯•è£…é¥°å™¨ï¼Œåœ¨å‡½æ•°æŠ›å‡ºå¼‚å¸¸æ—¶è‡ªåŠ¨é‡è¯•
    
    Args:
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        initial_delay: åˆå§‹å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        backoff_factor: å»¶è¿Ÿæ—¶é—´å¢é•¿å› å­
    """
    def decorator(func: Callable):
        def wrapper(*args, **kwargs):
            last_exception = None
            delay = initial_delay
            
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_retries:
                        logging.warning(f"API è°ƒç”¨å¤±è´¥ï¼Œç¬¬ {attempt + 1} æ¬¡é‡è¯•ï¼Œå»¶è¿Ÿ {delay:.1f} ç§’ã€‚é”™è¯¯: {str(e)}")
                        time.sleep(delay)
                        delay *= backoff_factor
                    else:
                        logging.error(f"API è°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}ã€‚é”™è¯¯: {str(e)}")
            
            # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼ŒæŠ›å‡ºæœ€åä¸€ä¸ªå¼‚å¸¸
            raise last_exception
        return wrapper
    return decorator


# ------------------------------
# å‘é‡æä¾›è€…ï¼ˆProviderï¼‰
# ------------------------------
class Embedder:
    def embed(self, texts: List[str], batch_size: int = 64) -> np.ndarray:
        raise NotImplementedError


class SiliconFlowEmbedder(Embedder):
    """ä½¿ç”¨ç¡…åŸºæµåŠ¨ OpenAIâ€‘å…¼å®¹ Embeddings APIã€‚

    éœ€è¦ç¯å¢ƒå˜é‡ï¼šSILICONFLOW_API_KEY
    é»˜è®¤ endpoint: https://api.siliconflow.cn/v1/embeddings
    æ¨¡å‹ï¼šBAAI/bge-m3ï¼ˆ4096 ç»´ï¼‰
    """
    def __init__(self, model: str = "BAAI/bge-m3", base_url: str = "https://api.siliconflow.cn/v1/embeddings"):
        import requests  # å»¶è¿Ÿå¯¼å…¥
        self.requests = requests
        self.model = model
        self.base_url = base_url.rstrip("/")
        self.api_key = os.environ.get("SILICONFLOW_API_KEY")
        if not self.api_key:
            raise RuntimeError("æœªæ‰¾åˆ°ç¯å¢ƒå˜é‡ SILICONFLOW_API_KEYã€‚è¯·å…ˆ export SILICONFLOW_API_KEY=...")

    @retry_with_backoff(max_retries=4, initial_delay=1.0, backoff_factor=2.0)
    def _make_api_request(self, chunk: List[str], headers: Dict[str, str]):
        """å†…éƒ¨æ–¹æ³•ï¼šå‘é€APIè¯·æ±‚ï¼Œå¸¦æœ‰é‡è¯•åŠŸèƒ½"""
        payload = {
            "model": self.model,
            "input": chunk,
        }
        resp = self.requests.post(self.base_url, headers=headers, json=payload, timeout=60)
        if resp.status_code != 200:
            raise RuntimeError(f"SiliconFlow API é”™è¯¯: {resp.status_code} {resp.text}")
        data = resp.json()
        # å…¼å®¹ OpenAI embeddings è¿”å›ç»“æ„
        embs = [d["embedding"] for d in data.get("data", [])]
        if len(embs) != len(chunk):
            raise RuntimeError("è¿”å›çš„ embedding æ•°é‡ä¸è¾“å…¥ä¸ä¸€è‡´")
        return embs

    def embed(self, texts: List[str], batch_size: int = 64) -> np.ndarray:
        out: List[List[float]] = []
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        for i in tqdm(range(0, len(texts), batch_size), desc="embedding", ncols=100):
            chunk = texts[i:i + batch_size]
            embs = self._make_api_request(chunk, headers)
            out.extend(embs)
        return np.asarray(out, dtype=np.float32)


class LocalSTEmbedder(Embedder):
    """æœ¬åœ° sentence-transformers ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆã€‚"""
    def __init__(self, model: str = "paraphrase-multilingual-MiniLM-L12-v2"):
        global _ST_MODEL
        if SentenceTransformer is None:
            raise RuntimeError("æœªå®‰è£… sentence-transformersï¼Œè¯· pip install sentence-transformers")
        if _ST_MODEL is None:
            _ST_MODEL = SentenceTransformer(model)
        self.model = _ST_MODEL

    def embed(self, texts: List[str], batch_size: int = 64) -> np.ndarray:
        embs = self.model.encode(texts, batch_size=batch_size, normalize_embeddings=True, show_progress_bar=True)
        return np.asarray(embs, dtype=np.float32)


# ------------------------------
# èšç±»ï¼ˆKMeans + è‡ªåŠ¨é€‰ Kï¼‰
# ------------------------------
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import normalize


def auto_kmeans(X: np.ndarray, k_min: int = 2, k_max: int = 12, random_state: int = 42) -> Tuple[np.ndarray, KMeans]:
    n = X.shape[0]
    if n < max(3, k_min):
        # æ ·æœ¬å¤ªå°‘ï¼Œå…¨éƒ¨å½’ä¸ºä¸€ç±»
        km = KMeans(n_clusters=1, n_init="auto", random_state=random_state)
        labels = km.fit_predict(X)
        return labels, km

    # å½’ä¸€åŒ–ä½™å¼¦ç©ºé—´ï¼ˆæ›´ç¨³ï¼‰
    Xn = normalize(X)

    best_k, best_score, best_km, best_labels = None, -1.0, None, None
    k_max = min(k_max, n - 1) if n > 2 else 1
    for k in range(max(2, k_min), max(3, k_max + 1)):
        km = KMeans(n_clusters=k, n_init="auto", random_state=random_state)
        labels = km.fit_predict(Xn)
        # æŸäº›èšç±»å¯èƒ½äº§ç”Ÿå•ä¸€ç°‡ï¼Œè·³è¿‡
        if len(set(labels)) < 2:
            continue
        try:
            score = silhouette_score(Xn, labels, metric="cosine")
        except Exception:
            continue
        if score > best_score:
            best_k, best_score, best_km, best_labels = k, score, km, labels

    if best_labels is None:
        # å…œåº•ï¼šå¼ºåˆ¶ 2 ç±»
        best_km = KMeans(n_clusters=2, n_init="auto", random_state=random_state)
        best_labels = best_km.fit_predict(Xn)

    return best_labels, best_km  # type: ignore


# ------------------------------
# HDBSCAN èšç±»ã€æ—¶é—´åˆ†æ¡¶ä¸è½¯å¸é™„
# ------------------------------
def is_short_phrase(s: str) -> bool:
    # æ¸…è½»åº¦ç¬¦å·ï¼Œåªè¦ 8 å­—ä»¥å†…æˆ– <=3 ä¸ªåˆ†è¯ï¼Œå°±è§†ä¸ºçŸ­è¯­
    t = re.sub(r"[ï¼Œã€‚ï¼ï¼Ÿ,.!?:ï¼š~ï½â€¦\s]+", "", s)
    if len(t) <= 8:
        return True
    toks = re.split(r"\s+", s.strip())
    return 0 < len(toks) <= 3

def detect_burst_chitchat(texts, msgs, idxs, window_minutes=30, min_users=3, min_count=3):
    buckets = bucket_indices_by_time(msgs, idxs, window_minutes)
    burst_mask = np.zeros(len(texts), dtype=bool)
    for (_, _), bidx in buckets.items():
        # ç»Ÿè®¡"çŸ­è¯­" -> {user_id set, indices}
        counter = {}
        for i in bidx:
            s = texts[i]
            if not s or not is_short_phrase(s):
                continue
            u = msgs[i].user_id or msgs[i].user_name or "?"
            key = s.strip()
            info = counter.setdefault(key, {"users": set(), "idx": []})
            info["users"].add(u)
            info["idx"].append(i)
        for key, info in counter.items():
            if len(info["users"]) >= min_users and len(info["idx"]) >= min_count:
                # æ‰“æ ‡ç­¾ä¸ºçˆ†å‘å¼é—²èŠ
                for i in info["idx"]:
                    burst_mask[i] = True
    return burst_mask

def bucket_indices_by_time(msgs: List[Message], idxs: List[int], window_minutes: int) -> Dict[Tuple[str, int], List[int]]:
    """
    æŒ‰ channel + æ—¶é—´çª—å£åˆ†æ¡¶ï¼›æ—  timestamp ç»Ÿä¸€æ”¾ -1ã€‚
    è¿”å›: {(channel, bucket_id): [indices...]}
    """
    buckets: Dict[Tuple[str, int], List[int]] = {}
    for i in idxs:
        m = msgs[i]
        channel = m.channel or "_global"
        if m.timestamp:
            minutes = int(m.timestamp.timestamp() // 60)
            bid = minutes // max(1, window_minutes)
        else:
            bid = -1
        buckets.setdefault((channel, bid), []).append(i)
    return buckets

def dynamic_min_cluster_size(n: int, env: str) -> int:
    if env != "auto":
        try:
            return int(env)
        except Exception:
            pass
    # sqrt ç­–ç•¥ï¼šæ ·æœ¬è¶Šå¤šé˜ˆå€¼è¶Šå¤§ï¼›æœ€å° 8ï¼Œæœ€å¤§ 32ï¼ˆå¯æŒ‰éœ€è°ƒï¼‰
    return int(np.clip(np.sqrt(max(n,1))*1.5, 8, 32))

def hdbscan_cluster(X: np.ndarray, min_cluster_size: int, min_samples: int) -> Tuple[np.ndarray, Any]:
    Xn = normalize(X)
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=max(2, int(min_cluster_size)),
        min_samples=max(1, int(min_samples)),
        metric="euclidean"  # åœ¨ L2 å½’ä¸€åŒ–åç­‰ä»·äº cosine
    )
    labels = clusterer.fit_predict(Xn)
    return labels, clusterer

def compute_centroids(X: np.ndarray, labels: np.ndarray) -> Dict[int, np.ndarray]:
    c2v: Dict[int, np.ndarray] = {}
    for c in sorted(set(labels)):
        if c < 0:
            continue
        idx = np.where(labels == c)[0]
        if len(idx) == 0:
            continue
        v = normalize(np.mean(X[idx], axis=0, keepdims=True)).ravel()
        c2v[int(c)] = v
    return c2v

def soft_attach_low_content(
    X: np.ndarray,
    labels: np.ndarray,
    low_idxs: List[int],
    msgs: List[Message],
    attach_sim_thr: float = 0.35,
) -> np.ndarray:
    """
    å°†ä½ä¿¡æ¯æ¶ˆæ¯/å™ªå£°æŒ‰ç°‡å¿ƒç›¸ä¼¼åº¦å¸é™„ï¼›è‹¥ reply_to æŒ‡å‘å·²åˆ†ç°‡çš„æ¶ˆæ¯ï¼Œåˆ™ä¼˜å…ˆè·Ÿéšã€‚
    æ·»åŠ ä¸‰ä¸ªé—¨æ§›ï¼šæ¦‚ç‡é˜ˆå€¼ã€ç›¸ä¼¼åº¦è½å·®ã€æ—¶é—´é—¨æ§›ã€‚
    """
    # è¯»å–å‚æ•°
    ATTACH_MARGIN = float(os.getenv("ATTACH_MARGIN", "0.07"))
    ATTACH_MAX_MINUTES = int(os.getenv("ATTACH_MAX_MINUTES", "90"))
    HDBSCAN_PROB_THR = float(os.getenv("HDBSCAN_PROB_THR", "0.45"))
    
    # å…ˆæŠŠæ‰€æœ‰éè´Ÿç°‡çš„è´¨å¿ƒç®—å¥½
    c2v = compute_centroids(X, labels)
    if not c2v:
        return labels
    C = np.stack(list(c2v.values()), axis=0)  # [C, D]
    C = normalize(C)
    cid_list = list(c2v.keys())
    Xn = normalize(X)

    # å»º id -> ç´¢å¼•æ˜ å°„ï¼Œä¾¿äºæŸ¥ reply_to
    id2idx = {m.id: i for i, m in enumerate(msgs)}
    for i in low_idxs:
        # è‹¥ reply_to å·²æœ‰ç°‡ï¼Œç›´æ¥è·Ÿéš
        rp = msgs[i].reply_to
        if rp and rp in id2idx:
            j = id2idx[rp]
            if labels[j] >= 0:
                labels[i] = labels[j]
                continue
        
        # å¦åˆ™çœ‹è´¨å¿ƒç›¸ä¼¼åº¦
        v = Xn[i:i+1]          # [1, D]
        sims = (v @ C.T).ravel()
        
        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿç›¸ä¼¼çš„ç°‡ï¼Œè·³è¿‡
        if np.max(sims) < attach_sim_thr:
            labels[i] = -1
            continue
            
        # æ£€æŸ¥ç›¸ä¼¼åº¦è½å·®ï¼ˆTop1 - Top2 >= ATTACH_MARGINï¼‰
        sorted_sims = np.sort(sims)[::-1]
        if len(sorted_sims) >= 2 and (sorted_sims[0] - sorted_sims[1]) < ATTACH_MARGIN:
            labels[i] = -1
            continue
            
        # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ç°‡
        k = int(np.argmax(sims))
        target_cid = cid_list[k]
        
        # æ£€æŸ¥æ—¶é—´é—¨æ§›ï¼ˆæ¶ˆæ¯æ—¶é—´ä¸ç°‡ä¸­ä½æ—¶é—´å·® <= ATTACH_MAX_MINUTESï¼‰
        if msgs[i].timestamp:
            # è®¡ç®—ç›®æ ‡ç°‡çš„æ—¶é—´ä¸­ä½æ•°
            cluster_times = []
            for j in range(len(labels)):
                if labels[j] == target_cid and msgs[j].timestamp:
                    cluster_times.append(msgs[j].timestamp)
            
            if cluster_times:
                median_time = np.median([t.timestamp() for t in cluster_times])
                time_diff = abs(msgs[i].timestamp.timestamp() - median_time) / 60  # è½¬æ¢ä¸ºåˆ†é’Ÿ
                if time_diff > ATTACH_MAX_MINUTES:
                    labels[i] = -1
                    continue
        
        # æ‰€æœ‰é—¨æ§›éƒ½é€šè¿‡ï¼Œåˆ†é…åˆ°è¯¥ç°‡
        labels[i] = target_cid
        
    return labels


from sklearn.neighbors import NearestNeighbors

def knn_label_propagation(
    X: np.ndarray,
    labels: np.ndarray,
    candidate_idxs: List[int],
    msgs: List[Message],
    k: int = 5,
    min_in_cluster: int = 2,
    mean_sim_thr: float = 0.34,
    attach_max_minutes: int = 180,
    cluster_median_ts: Optional[Dict[int,int]] = None,
) -> np.ndarray:
    Xn = normalize(X)
    lbl = labels.copy()
    # å·²æœ‰ç°‡çš„æ ·æœ¬ä½œä¸º"å›¾åº“"
    base_idx = np.where(lbl >= 0)[0]
    if len(base_idx) == 0 or len(candidate_idxs) == 0:
        return lbl
    nn = NearestNeighbors(n_neighbors=min(k, len(base_idx)), metric="cosine")
    nn.fit(Xn[base_idx])
    dists, nbrs = nn.kneighbors(Xn[candidate_idxs], return_distance=True)
    # cosine è·ç¦» -> ç›¸ä¼¼åº¦
    sims = 1.0 - dists
    for row, idx in enumerate(candidate_idxs):
        nbr_ids = base_idx[nbrs[row]]
        nbr_labels = lbl[nbr_ids]
        # å–é‚»å±…ä¸­å·²æ ‡ç°‡çš„
        mask = nbr_labels >= 0
        if mask.sum() == 0:
            continue
        # æŠ•ç¥¨ï¼šæ‰¾å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç°‡ï¼Œå¹¶è®¡ç®—è¯¥ç°‡é‚»å±…çš„å¹³å‡ç›¸ä¼¼åº¦
        labs, counts = np.unique(nbr_labels[mask], return_counts=True)
        best_lab = int(labs[np.argmax(counts)])
        in_cluster = (nbr_labels[mask] == best_lab)
        mean_sim = float(sims[row][mask][in_cluster].mean())
        if counts.max() >= min_in_cluster and mean_sim >= mean_sim_thr:
            # æ—¶é—´é—¨æ§›
            pass_time = True
            if cluster_median_ts and msgs[idx].timestamp:
                cts = cluster_median_ts.get(best_lab)
                if cts is not None:
                    dt = abs(int(msgs[idx].timestamp.timestamp()) - cts) / 60.0
                    pass_time = dt <= float(attach_max_minutes)
            if pass_time:
                lbl[idx] = best_lab
    return lbl


# ------------------------------
# ä¸»é¢˜å‘½åï¼ˆTFâ€‘IDF æå–å…³é”®è¯ï¼‰
# ------------------------------
from sklearn.feature_extraction.text import TfidfVectorizer


def _jieba_tokenize(s: str) -> List[str]:
    return [w.strip() for w in jieba.cut(s) if w.strip()]

DOMAIN_STOPWORDS = set("""
cq at reply http https com www img jpg png pdf doc ppt rar zip
å“ˆå“ˆ å“ˆå“ˆå“ˆ å—¯ å•Š å“¦ å‘¢ å‘€ å‘ƒ æ˜¯çš„ å¯ä»¥ ä¸è¡Œ å¥½ è¡Œ ï¼Ÿ ?
""".split())

def build_vectorizer(use_jieba: bool = True) -> TfidfVectorizer:
    if use_jieba and HAS_JIEBA:
        return TfidfVectorizer(
            tokenizer=_jieba_tokenize,
            token_pattern=None,
            lowercase=False,
            ngram_range=(1, 2),
            max_features=50000,
            stop_words=list(DOMAIN_STOPWORDS)
        )
    # é€€åŒ–ï¼šä¸­æ–‡å­—ç¬¦ n-gramï¼ˆä¸ä¾èµ–åˆ†è¯ä¹Ÿèƒ½å‡‘åˆï¼‰
    return TfidfVectorizer(
        analyzer="char",
        ngram_range=(2, 4),
        max_features=50000,
    )

def ctfidf_terms_for_clusters(cluster_docs: List[str], topk: int = 8, use_jieba: bool = True) -> List[List[str]]:
    """
    c-TF-IDF: æŠŠæ¯ä¸ªç°‡çš„æ–‡æœ¬æ‹¼æ¥æˆ"å¤§æ–‡æ¡£"ï¼Œå†åš TF-IDFã€‚
    è¿”å›ï¼šæ¯ä¸ªç°‡çš„å…³é”®è¯åˆ—è¡¨ã€‚
    """
    if not cluster_docs:
        return []
    vec = build_vectorizer(use_jieba=use_jieba)
    X = vec.fit_transform(cluster_docs)  # shape [C, V]
    vocab = np.array(vec.get_feature_names_out())
    # å–æ¯ä¸ªç°‡æ–‡æ¡£çš„ topk è¯
    out: List[List[str]] = []
    for i in range(X.shape[0]):
        row = X.getrow(i)
        scores = row.toarray().ravel()
        idx = np.argsort(-scores)[:topk]
        terms = [t for t in vocab[idx] if t not in DOMAIN_STOPWORDS][:topk]
        out.append(terms)
    return out


def make_topic_name(terms: List[str], fallback: str = "è®¨è®ºä¸»é¢˜") -> str:
    if not terms:
        return fallback
    return " / ".join(terms[:4])


# ------------------------------
# ä»£è¡¨æ¶ˆæ¯ï¼ˆç°‡å¿ƒï¼‰
# ------------------------------

def representative_indices(X: np.ndarray, labels: np.ndarray, topk: int = 5) -> Dict[int, List[int]]:
    reps: Dict[int, List[int]] = {}
    for c in sorted(set(labels)):
        idx = np.where(labels == c)[0]
        if len(idx) == 0:
            reps[c] = []
            continue
        # ç”¨ç°‡å†…å¹³å‡å‘é‡åšâ€œä¸­å¿ƒâ€ï¼Œé€‰ä½™å¼¦ç›¸ä¼¼åº¦æœ€é«˜çš„å‰ k æ¡
        center = normalize(np.mean(X[idx], axis=0, keepdims=True))
        Xi = normalize(X[idx])
        sims = (Xi @ center.T).ravel()
        top_idx = idx[np.argsort(-sims)[:topk]]
        reps[c] = top_idx.tolist()
    return reps


# ------------------------------
# ä¸»æµç¨‹
# ------------------------------

def run(
    input_path: str,
    provider: str = "siliconflow",
    model: str = "BAAI/bge-m3",
    output_dir: str = "./out",
    k_min: int = 2,
    k_max: int = 12,
    batch_size: int = 64,
    use_jieba: bool = True,
) -> None:
    os.makedirs(output_dir, exist_ok=True)
    logging.info("è¯»å–è¾“å…¥â€¦â€¦")
    msgs = read_messages(input_path)
    if not msgs:
        raise SystemExit("æ²¡æœ‰è¯»åˆ°ä»»ä½•æ¶ˆæ¯ã€‚è¯·æ£€æŸ¥ --input")

    logging.info("æ„å»ºæ–‡æœ¬â€¦â€¦")
    texts = [build_message_text(m) for m in msgs]

    logging.info("å‘é‡è®¡ç®—ï¼ˆprovider=%s, model=%sï¼‰â€¦â€¦", provider, model)
    if provider.lower() == "siliconflow":
        embedder: Embedder = SiliconFlowEmbedder(model=model)
    elif provider.lower() == "local":
        embedder = LocalSTEmbedder(model=model)
    else:
        raise SystemExit("provider ä»…æ”¯æŒ siliconflow / local")

    X = embedder.embed(texts, batch_size=batch_size)
    if not np.all(np.isfinite(X)):
        raise RuntimeError("Embedding ä¸­å­˜åœ¨ inf/nanï¼Œè¯·æ£€æŸ¥è¾“å…¥æˆ– provider è¿”å›å€¼ã€‚")

    # è¯»å– .env ä¸­çš„å‚æ•°ï¼ˆå¯è¢« CLI å‚æ•°éƒ¨åˆ†è¦†ç›–/æ— å½±å“ï¼‰
    WINDOW_MINUTES = int(os.getenv("WINDOW_MINUTES", "30"))
    H_MIN_CLUSTER = int(os.getenv("HDBSCAN_MIN_CLUSTER_SIZE", "20"))
    H_MIN_SAMPLES = int(os.getenv("HDBSCAN_MIN_SAMPLES", "5"))
    ATTACH_SIM_THR = float(os.getenv("ATTACH_SIM_THR", "0.35"))
    ATTACH_MARGIN = float(os.getenv("ATTACH_MARGIN", "0.07"))
    ATTACH_MAX_MINUTES = int(os.getenv("ATTACH_MAX_MINUTES", "90"))
    MAX_RECLUSTER_SIZE = int(os.getenv("MAX_RECLUSTER_SIZE", "80"))
    
    # æ ‡è®°ä½ä¿¡æ¯æ ·æœ¬ï¼ˆä¸ç›´æ¥å‚ä¸èšç±»ï¼‰
    low_mask = np.array([is_low_content(t) for t in texts], dtype=bool)
    
    # è®¡ç®—é—²èŠä¸­å¿ƒï¼ˆç”¨äºæ£€æµ‹ä¸é—²èŠç›¸ä¼¼çš„å†…å®¹ï¼‰
    CHITCHAT_SIM_THR = float(os.getenv("CHITCHAT_SIM_THR", "0.68"))
    # ä½¿ç”¨ä½ä¿¡æ¯å†…å®¹çš„å¹³å‡å‘é‡ä½œä¸ºé—²èŠä¸­å¿ƒ
    low_content_indices = [i for i in range(len(texts)) if low_mask[i]]
    if low_content_indices:
        chit_center = normalize(np.mean(X[low_content_indices], axis=0, keepdims=True)).ravel()
    else:
        chit_center = np.zeros(X.shape[1])
    
    # æ£€æµ‹ä¸é—²èŠä¸­å¿ƒç›¸ä¼¼çš„å†…å®¹
    def is_chitchat_by_vector(vec, center, thr):
        if np.all(center == 0):  # å¦‚æœæ²¡æœ‰é—²èŠä¸­å¿ƒ
            return False
        sim = np.dot(vec, center)
        return sim >= thr
    
    Xn_all = normalize(X)
    chit_mask = np.array([is_chitchat_by_vector(Xn_all[i:i+1], chit_center, CHITCHAT_SIM_THR) for i in range(len(texts))])
    
    # æ–°å¢ï¼šçˆ†å‘å¼çŸ­è¯­æ£€æµ‹ï¼ˆåŒæ¡¶å†…å¤šäººå¤è¯»ï¼‰
    BURST_MIN_USERS = int(os.getenv("BURST_MIN_USERS", "3"))
    BURST_MIN_COUNT = int(os.getenv("BURST_MIN_COUNT", "3"))
    burst_mask = detect_burst_chitchat(texts, msgs, list(range(len(texts))), window_minutes=WINDOW_MINUTES,
                                       min_users=BURST_MIN_USERS, min_count=BURST_MIN_COUNT)
    
    anchor_mask = np.array([has_anchor(t) for t in texts], dtype=bool)
    
    # æ ¸å¿ƒé›†åˆï¼šä¸æ˜¯ä½ä¿¡æ¯ï¼Œä¸”ï¼ˆä¸æ˜¯é—²èŠ/çˆ†å‘ æˆ– æœ‰é”šç‚¹ï¼‰
    core_mask = (~low_mask) & ( (~(chit_mask | burst_mask)) | anchor_mask )
    border_mask = (~low_mask) & (~core_mask)   # å…¶ä½™éä½ä¿¡æ¯çš„ï¼Œåç»­ç”¨ kNN è¡¥å›
    
    # ç¡®ä¿æ•°ç»„æ˜¯ä¸€ç»´çš„
    core_mask = core_mask.flatten()
    border_mask = border_mask.flatten()
    
    valid_idxs  = [i for i in range(len(texts)) if core_mask[i]]
    border_idxs = [i for i in range(len(texts)) if border_mask[i]]
    logging.info("æ ¸å¿ƒ: %d | è¾¹ç•Œ: %d | ä½ä¿¡æ¯: %d",
                 len(valid_idxs), len(border_idxs), int(low_mask.sum()))

    # 1) æ—¶é—´åˆ†æ¡¶ + HDBSCAN
    logging.info("æŒ‰ channel+%dåˆ†é’Ÿ åˆ†æ¡¶èšç±»ï¼ˆHDBSCAN ä¼˜å…ˆï¼‰â€¦â€¦", WINDOW_MINUTES)
    global_labels = np.full(len(texts), -1, dtype=int)
    buckets = bucket_indices_by_time(msgs, valid_idxs, WINDOW_MINUTES)
    next_cid = 0
    for (ch, bid), idxs in buckets.items():
        subX = X[idxs]
        mcs = dynamic_min_cluster_size(len(idxs), os.getenv("DYNAMIC_MIN_CLUSTER","auto"))
        if len(idxs) < max(mcs, 10):
            sub_labels, _ = auto_kmeans(subX, k_min=2, k_max=min(6, len(idxs)))
        else:
            sub_labels, sub_clusterer = hdbscan_cluster(subX, mcs, H_MIN_SAMPLES)
            try:
                probs = sub_clusterer.probabilities_
                HDBSCAN_PROB_THR = float(os.getenv("HDBSCAN_PROB_THR", "0.38"))
                sub_labels = np.where(probs >= HDBSCAN_PROB_THR, sub_labels, -1)
            except Exception:
                pass
            # è‹¥å™ªå£°å æ¯”è¿‡å¤§ï¼ˆ>60%ï¼‰ï¼Œè¯´æ˜è¿‡ä¸¥ â†’ å›é€€ KMeans
            if (sub_labels < 0).mean() > 0.60:
                sub_labels, _ = auto_kmeans(subX, k_min=2, k_max=min(6, len(idxs)))
        # æ˜ å°„åˆ°å…¨å±€ label
        unique = sorted(set([l for l in sub_labels if l >= 0]))
        mapping = {l: (next_cid + i) for i, l in enumerate(unique)}
        for j, lab in zip(idxs, sub_labels):
            if lab >= 0:
                global_labels[j] = mapping[lab]
        next_cid += len(unique)

    # 2) é€’å½’ç»†åˆ†å·¨æ— éœ¸ç°‡ï¼ˆç®€å•ä¸€è½®ï¼‰
    for c in sorted(set(global_labels)):
        if c < 0:
            continue
        members = np.where(global_labels == c)[0]
        if len(members) > MAX_RECLUSTER_SIZE:
            sub_labels, _ = auto_kmeans(X[members], k_min=2, k_max=min(8, len(members)-1))
            # é‡æ ‡å·
            uniq = sorted(set(sub_labels))
            remap = {l: (next_cid + i) for i, l in enumerate(uniq)}
            for k, lab in zip(members, sub_labels):
                global_labels[k] = remap[lab]
            next_cid += len(uniq)

    # 3) å…ˆå¯¹"è¾¹ç•Œ+å™ªå£°"ç”¨ kNN æ ‡ç­¾ä¼ æ’­è¡¥å…¨ï¼Œå†ç”¨è´¨å¿ƒè½¯å¸é™„å…œåº•
    # ç¡®ä¿ border_mask ä¸ global_labels å½¢çŠ¶ä¸€è‡´
    if len(border_mask) != len(global_labels):
        border_mask = border_mask[:len(global_labels)]
    
    # è¿›é˜¶ï¼šæ˜ç¡®æ ‡æ³¨é—²èŠï¼ŒæŠŠçº¯é—²èŠæ‰“æˆå›ºå®šæ ‡ç­¾-2
    for i in range(len(texts)):
        if chit_mask[i] and not anchor_mask[i] and not low_mask[i]:
            global_labels[i] = -2  # æ ‡è®°ä¸ºé—²èŠ
    # ä¿®æ”¹KNNä¼ æ’­çš„candidate_idxsæ„é€ ï¼Œåªå¯¹"éä½ä¿¡æ¯ã€éé—²èŠ"çš„æœªå½’å±/è¾¹ç•Œæ ·æœ¬åšä¼ æ’­
    cand = [i for i in range(len(texts))
            if ((global_labels[i] < 0) or border_mask[i])
            and (global_labels[i] != -2)      # è·³è¿‡å·²æ ‡è®°çš„é—²èŠæ ·æœ¬
            and (not low_mask[i])             # æ’é™¤ä½ä¿¡æ¯
            and (not (chit_mask[i] and not anchor_mask[i]))]  # æ’é™¤çº¯é—²èŠ
    
    # è®¡ç®—æ¯ä¸ªç°‡çš„æ—¶é—´ä¸­ä½æ•°ï¼Œç”¨äºæ—¶é—´é—¨æ§›æ£€æŸ¥
    cluster_median_ts = {}
    for c in sorted(set(global_labels)):
        if c < 0:
            continue
        cluster_times = []
        for i in range(len(global_labels)):
            if global_labels[i] == c and msgs[i].timestamp:
                cluster_times.append(msgs[i].timestamp.timestamp())
        if cluster_times:
            cluster_median_ts[c] = int(np.median(cluster_times))
    
    # 3.1 kNN æ ‡ç­¾ä¼ æ’­ï¼ˆå…ˆè¡¥ä¸€è½®ï¼‰
    KNN_K = int(os.getenv("KNN_K","5"))
    KNN_MIN_IN_CLUSTER = int(os.getenv("KNN_MIN_IN_CLUSTER","2"))
    KNN_MEAN_SIM_THR = float(os.getenv("KNN_MEAN_SIM_THR","0.34"))
    global_labels = knn_label_propagation(
        X, global_labels, cand, msgs,
        k=KNN_K, min_in_cluster=KNN_MIN_IN_CLUSTER, mean_sim_thr=KNN_MEAN_SIM_THR,
        attach_max_minutes=ATTACH_MAX_MINUTES, cluster_median_ts=cluster_median_ts
    )
    
    # 3.2 ä»æœªå½’å±çš„å†ç”¨è´¨å¿ƒè½¯å¸é™„å…œåº•ï¼ˆæ›´å®½æ¾ï¼‰
    # è½¯å¸é™„å‰å†è¿‡æ»¤ï¼Œåªå…è®¸éä½ä¿¡æ¯ã€éé—²èŠè¿›å…¥è½¯å¸é™„å€™é€‰
    remain = [i for i in range(len(texts))
              if global_labels[i] < 0
              and (global_labels[i] != -2)      # è·³è¿‡å·²æ ‡è®°çš„é—²èŠæ ·æœ¬
              and (not low_mask[i])
              and (not (chit_mask[i] and not anchor_mask[i]))]
    global_labels = soft_attach_low_content(
        X=X, labels=global_labels, low_idxs=remain, msgs=msgs,
        attach_sim_thr=ATTACH_SIM_THR
    )
    
    # 3.3 äºŒæ¬¡å‘ç°ï¼šå¯¹å‰©ä½™æœªå½’å±æ ·æœ¬åšä¸€æ¬¡å° HDBSCAN
    unassigned = np.where(global_labels < 0)[0]
    if len(unassigned) >= 12:
        mcs2 = max(6, int(dynamic_min_cluster_size(len(unassigned), "auto") * 0.6))
        sub_labels2, sub_clusterer2 = hdbscan_cluster(X[unassigned], mcs2, max(3, H_MIN_SAMPLES-1))
        # æŠŠ -1 ä»¥å¤–çš„æ˜ å°„åˆ°æ–°çš„å…¨å±€ç°‡
        uniq2 = sorted(set([l for l in sub_labels2 if l >= 0]))
        mapping2 = {l: (next_cid + i) for i, l in enumerate(uniq2)}
        for u, lab in zip(unassigned, sub_labels2):
            if lab >= 0:
                global_labels[u] = mapping2[lab]
        next_cid += len(uniq2)

    labels = global_labels
    reps = representative_indices(X, labels, topk=5)

    logging.info("ä¸»é¢˜å‘½åï¼ˆc-TF-IDFï¼‰â€¦â€¦")
    df = pd.DataFrame({
        "id": [m.id for m in msgs],
        "timestamp": [m.timestamp.isoformat() if m.timestamp else None for m in msgs],
        "user_id": [m.user_id for m in msgs],
        "user_name": [m.user_name for m in msgs],
        "channel": [m.channel for m in msgs],
        "text": texts,
        "label": labels,
    })

    clusters: List[Dict[str, Any]] = []
    valid_clusters = [c for c in sorted(df["label"].unique()) if c >= 0]
    # æ¯ä¸ªç°‡æ‹¼æ¥æ–‡æ¡£åš c-TF-IDF
    docs = [" ".join(df[df.label == c]["text"].tolist()) for c in valid_clusters]
    terms_list = ctfidf_terms_for_clusters(docs, topk=8, use_jieba=use_jieba)
    c2terms = {c: terms for c, terms in zip(valid_clusters, terms_list)}

    for c in valid_clusters:
        sub = df[df["label"] == c]
        terms = c2terms.get(c, [])
        topic = make_topic_name(terms, fallback=f"ä¸»é¢˜#{c}")
        rep_rows = sub.loc[reps.get(c, [])]
        clusters.append({
            "cluster_id": int(c),
            "topic": topic,
            "size": int(len(sub)),
            "keywords": terms[:8],
            "representative_messages": [
                {
                    "id": r["id"],
                    "user": r["user_name"],
                    "timestamp": r["timestamp"],
                    "text": r["text"],
                }
                for _, r in rep_rows.iterrows()
            ],
            "message_ids": sub["id"].tolist(),
        })

    # å¯¼å‡º
    out_jsonl = os.path.join(output_dir, "messages_with_labels.jsonl")
    out_clusters = os.path.join(output_dir, "clusters.json")

    logging.info("å†™å‡ºï¼š%s", out_jsonl)
    with open(out_jsonl, "w", encoding="utf-8") as f:
        for _, row in df.iterrows():
            f.write(json.dumps(row.to_dict(), ensure_ascii=False) + "\n")

    logging.info("å†™å‡ºï¼š%s", out_clusters)
    with open(out_clusters, "w", encoding="utf-8") as f:
        json.dump(clusters, f, ensure_ascii=False, indent=2)

    total = len(df)
    noise_cnt = int((df["label"] == -1).sum())
    chatter_cnt = int((df["label"] == -2).sum()) if (-2 in df["label"].unique()) else 0
    border_joined = len([i for i in border_idxs if labels[i] >= 0])
    logging.info("æ€»æ•°: %d | ä¸»é¢˜ç°‡: %d | å™ªå£°: %d | é—²èŠ: %d | è¾¹ç•Œè¡¥å›: %d",
                 total, len([c for c in set(labels) if c >= 0]), noise_cnt, chatter_cnt, border_joined)
    
    logging.info("å®Œæˆã€‚å…± %d æ¡æ¶ˆæ¯ï¼Œè¢«åˆ†æˆ %d ä¸ªä¸»é¢˜ç»„ã€‚", len(df), len(clusters))


# ------------------------------
# CLI
# ------------------------------

def parse_args() -> argparse.Namespace:
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
    input_path = os.getenv("INPUT", "./chat.jsonl")
    provider = os.getenv("PROVIDER", "siliconflow")
    model = os.getenv("MODEL", "BAAI/bge-m3")
    output_dir = os.getenv("OUTPUT", "./out")
    k_min = int(os.getenv("K_MIN", "2"))
    k_max = int(os.getenv("K_MAX", "12"))
    batch_size = int(os.getenv("BATCH_SIZE", "64"))
    
    p = argparse.ArgumentParser(
        description="æŠŠèŠå¤©è®°å½•åˆ†ç»„ä¸ºä¸»é¢˜ï¼ˆSiliconFlow BAAI/bge-m3 4096ç»´ï¼‰",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    p.add_argument("--input", default=input_path, help="è¾“å…¥ .jsonl æ–‡ä»¶æˆ–åŒ…å« .jsonl çš„ç›®å½•")
    p.add_argument("--provider", default=provider, choices=["siliconflow", "local"], help="å‘é‡æä¾›è€…")
    p.add_argument("--model", default=model, help="å‘é‡æ¨¡å‹åï¼ˆsiliconflow/localï¼‰")
    p.add_argument("--output", default=output_dir, help="è¾“å‡ºç›®å½•")
    p.add_argument("--k_min", type=int, default=k_min, help="KMeans æœ€å°ç°‡æ•°")
    p.add_argument("--k_max", type=int, default=k_max, help="KMeans æœ€å¤§ç°‡æ•°")
    p.add_argument("--batch_size", type=int, default=batch_size, help="Embedding æ‰¹å¤§å°")
    p.add_argument("--no_jieba", action="store_true", help="ä¸ä½¿ç”¨ jiebaï¼ˆå¼ºåˆ¶ä½¿ç”¨ char ngramï¼‰")
    p.add_argument("--log_level", default="INFO", help="æ—¥å¿—ç­‰çº§ï¼šDEBUG/INFO/WARN/ERROR")
    return p.parse_args()


def main() -> None:
    load_dotenv()  # è¯»å– .env å‚æ•°ï¼ˆè‹¥å­˜åœ¨ï¼‰
    args = parse_args()
    logging.basicConfig(level=getattr(logging, args.log_level.upper(), logging.INFO), format="%(asctime)s | %(levelname)s | %(message)s")
    run(
        input_path=args.input,
        provider=args.provider,
        model=args.model,
        output_dir=args.output,
        k_min=args.k_min,
        k_max=args.k_max,
        batch_size=args.batch_size,
        use_jieba=(not args.no_jieba),
    )


if __name__ == "__main__":
    main()
